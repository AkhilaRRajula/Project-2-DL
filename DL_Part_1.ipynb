{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeC7yzaybFvb"
      },
      "outputs": [],
      "source": [
        "!pip install -U portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMSrA7r8bOJt"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output stored in csv"
      ],
      "metadata": {
        "id": "0lb4XDcLeBst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Loading the datasets\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Neural Network Definition\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the image\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = torch.relu(layer(x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x\n",
        "\n",
        "# Experiment configurations\n",
        "configurations = [\n",
        "    {\"layer_sizes\": [28*28, 64, 10], \"optimizer\": \"SGD\", \"lr\": 0.01, \"batch_size\": 64},\n",
        "    {\"layer_sizes\": [28*28, 128, 10], \"optimizer\": \"Adam\", \"lr\": 0.001, \"batch_size\": 50},\n",
        "    {\"layer_sizes\": [28*28, 64, 32, 10], \"optimizer\": \"RMSprop\", \"lr\": 0.0005, \"batch_size\": 100},\n",
        "    {\"layer_sizes\": [28*28, 128, 64, 10], \"optimizer\": \"SGD\", \"lr\": 0.01, \"batch_size\": 32},\n",
        "    {\"layer_sizes\": [28*28, 256, 128, 64, 10], \"optimizer\": \"Adam\", \"lr\": 0.001, \"batch_size\": 128},\n",
        "    {\"layer_sizes\": [28*28, 256, 128, 10], \"optimizer\": \"RMSprop\", \"lr\": 0.0001, \"batch_size\": 75},\n",
        "    {\"layer_sizes\": [28*28, 512, 256, 128, 10], \"optimizer\": \"SGD\", \"lr\": 0.005, \"batch_size\": 60},\n",
        "    {\"layer_sizes\": [28*28, 512, 256, 10], \"optimizer\": \"Adam\", \"lr\": 0.001, \"batch_size\": 45},\n",
        "    {\"layer_sizes\": [28*28, 128, 64, 32, 16, 10], \"optimizer\": \"RMSprop\", \"lr\": 0.0005, \"batch_size\": 30},\n",
        "    {\"layer_sizes\": [28*28, 1024, 512, 256, 128, 64, 10], \"optimizer\": \"SGD\", \"lr\": 0.002, \"batch_size\": 90},\n",
        "    {\"layer_sizes\": [28*28, 32, 10], \"optimizer\": \"SGD\", \"lr\": 0.005, \"batch_size\": 128},\n",
        "    {\"layer_sizes\": [28*28, 256, 128, 64, 10], \"optimizer\": \"Adam\", \"lr\": 0.0005, \"batch_size\": 100},\n",
        "    {\"layer_sizes\": [28*28, 64, 10], \"optimizer\": \"RMSprop\", \"lr\": 0.0001, \"batch_size\": 32},\n",
        "    {\"layer_sizes\": [28*28, 512, 256, 10], \"optimizer\": \"SGD\", \"lr\": 0.001, \"batch_size\": 75},\n",
        "    {\"layer_sizes\": [28*28, 512, 256, 10], \"optimizer\": \"Adam\", \"lr\": 0.0005, \"batch_size\": 45},\n",
        "    {\"layer_sizes\": [28*28, 128, 64, 10], \"optimizer\": \"RMSprop\", \"lr\": 0.0001, \"batch_size\": 60},\n",
        "    {\"layer_sizes\": [28*28, 1024, 512, 256, 64, 10], \"optimizer\": \"SGD\", \"lr\": 0.0015, \"batch_size\": 90},\n",
        "    {\"layer_sizes\": [28*28, 64, 32, 10], \"optimizer\": \"Adam\", \"lr\": 0.0005, \"batch_size\": 50},\n",
        "    {\"layer_sizes\": [28*28, 256, 128, 10], \"optimizer\": \"RMSprop\", \"lr\": 0.0001, \"batch_size\": 75},\n",
        "    {\"layer_sizes\": [28*28, 512, 256, 128, 10], \"optimizer\": \"SGD\", \"lr\": 0.001, \"batch_size\": 60}\n",
        "]\n",
        "\n",
        "import csv\n",
        "\n",
        "# Train and evaluate each configuration\n",
        "results = []\n",
        "for idx, config in enumerate(configurations):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "    model = MLP(config[\"layer_sizes\"])\n",
        "\n",
        "    if config[\"optimizer\"] == \"SGD\":\n",
        "        optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
        "    elif config[\"optimizer\"] == \"Adam\":\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "    elif config[\"optimizer\"] == \"RMSprop\":\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 20  # Adjust the number of epochs for detailed analysis\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for data in train_loader:\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluation after each epoch\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                images, labels = data\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        # Append results for each epoch\n",
        "        results.append({\n",
        "            \"Configuration\": idx + 1,\n",
        "            \"Layer Sizes\": config[\"layer_sizes\"],\n",
        "            \"Optimizer\": config[\"optimizer\"],\n",
        "            \"Learning Rate\": config[\"lr\"],\n",
        "            \"Batch Size\": config[\"batch_size\"],\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Accuracy\": f'{accuracy:.2f}%'\n",
        "        })\n",
        "\n",
        "# Write results to a CSV file\n",
        "with open('results.csv', mode='w', newline='') as file:\n",
        "    fieldnames = ['Configuration', 'Layer Sizes', 'Optimizer', 'Learning Rate', 'Batch Size', 'Epoch', 'Accuracy']\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for result in results:\n",
        "        writer.writerow(result)"
      ],
      "metadata": {
        "id": "BDKMs4k3eAqt"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}