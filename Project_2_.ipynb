{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Loading the datasets\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Neural Network Definition\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the image\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = torch.relu(layer(x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x\n",
        "\n",
        "# Experiment configurations\n",
        "configurations = [\n",
        "    # Your configurations\n",
        "]\n",
        "\n",
        "import csv\n",
        "\n",
        "# Train and evaluate each configuration\n",
        "results = []\n",
        "for idx, config in enumerate(configurations):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "    model = MLP(config[\"layer_sizes\"])\n",
        "\n",
        "    if config[\"optimizer\"] == \"SGD\":\n",
        "        optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
        "    elif config[\"optimizer\"] == \"Adam\":\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "    elif config[\"optimizer\"] == \"RMSprop\":\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 25  # Adjust the number of epochs for detailed analysis\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for data in train_loader:\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluation after each epoch\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                images, labels = data\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        # Append results for each epoch\n",
        "        if epoch == num_epochs - 1:  # If last epoch, add the result to the list\n",
        "            results.append({\n",
        "                \"Configuration\": idx + 1,\n",
        "                \"Layer Sizes\": config[\"layer_sizes\"],\n",
        "                \"Optimizer\": config[\"optimizer\"],\n",
        "                \"Learning Rate\": config[\"lr\"],\n",
        "                \"Batch Size\": config[\"batch_size\"],\n",
        "                \"Epoch\": epoch + 1,\n",
        "                \"Accuracy\": f'{accuracy:.2f}%'\n",
        "            })\n",
        "\n",
        "# Write results to a CSV file\n",
        "with open('final_results.csv', mode='w', newline='') as file:\n",
        "    fieldnames = ['Configuration', 'Layer Sizes', 'Optimizer', 'Learning Rate', 'Batch Size', 'Epoch', 'Accuracy']\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for result in results:\n",
        "        writer.writerow(result)\n"
      ],
      "metadata": {
        "id": "lFdLxz4nLw1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9Bj3ed6rh-k",
        "outputId": "efa56ab9-e19d-42ae-aac3-9b7b32cc6d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 147s 195ms/step - loss: 0.4690 - accuracy: 0.8312 - val_loss: 0.3292 - val_accuracy: 0.8811\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 144s 192ms/step - loss: 0.2972 - accuracy: 0.8923 - val_loss: 0.2713 - val_accuracy: 0.8997\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 145s 193ms/step - loss: 0.2497 - accuracy: 0.9091 - val_loss: 0.2422 - val_accuracy: 0.9108\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 141s 188ms/step - loss: 0.2182 - accuracy: 0.9191 - val_loss: 0.2156 - val_accuracy: 0.9244\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 139s 185ms/step - loss: 0.1958 - accuracy: 0.9278 - val_loss: 0.2168 - val_accuracy: 0.9233\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 139s 186ms/step - loss: 0.1728 - accuracy: 0.9357 - val_loss: 0.2182 - val_accuracy: 0.9213\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 140s 186ms/step - loss: 0.1584 - accuracy: 0.9406 - val_loss: 0.2150 - val_accuracy: 0.9227\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 138s 185ms/step - loss: 0.1406 - accuracy: 0.9471 - val_loss: 0.2082 - val_accuracy: 0.9293\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 139s 186ms/step - loss: 0.1251 - accuracy: 0.9538 - val_loss: 0.2070 - val_accuracy: 0.9256\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 137s 183ms/step - loss: 0.1151 - accuracy: 0.9573 - val_loss: 0.2206 - val_accuracy: 0.9223\n",
            "313/313 [==============================] - 8s 25ms/step - loss: 0.2365 - accuracy: 0.9180\n",
            "Test accuracy: 0.92\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 278s 370ms/step - loss: 0.8746 - accuracy: 0.6946 - val_loss: 0.5808 - val_accuracy: 0.7885\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 268s 358ms/step - loss: 0.5693 - accuracy: 0.7894 - val_loss: 0.5120 - val_accuracy: 0.8187\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 271s 361ms/step - loss: 0.5017 - accuracy: 0.8185 - val_loss: 0.4768 - val_accuracy: 0.8224\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 269s 359ms/step - loss: 0.4551 - accuracy: 0.8381 - val_loss: 0.4484 - val_accuracy: 0.8410\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 264s 352ms/step - loss: 0.4201 - accuracy: 0.8510 - val_loss: 0.4072 - val_accuracy: 0.8548\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 269s 359ms/step - loss: 0.3935 - accuracy: 0.8611 - val_loss: 0.3909 - val_accuracy: 0.8596\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 268s 358ms/step - loss: 0.3715 - accuracy: 0.8687 - val_loss: 0.3605 - val_accuracy: 0.8719\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 296s 394ms/step - loss: 0.3547 - accuracy: 0.8741 - val_loss: 0.3443 - val_accuracy: 0.8780\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 288s 384ms/step - loss: 0.3390 - accuracy: 0.8786 - val_loss: 0.3366 - val_accuracy: 0.8807\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 296s 395ms/step - loss: 0.3254 - accuracy: 0.8829 - val_loss: 0.3238 - val_accuracy: 0.8852\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.3418 - accuracy: 0.8761\n",
            "Test accuracy: 0.88\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 142s 188ms/step - loss: 0.4714 - accuracy: 0.8263 - val_loss: 0.3102 - val_accuracy: 0.8879\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 143s 190ms/step - loss: 0.2970 - accuracy: 0.8909 - val_loss: 0.2626 - val_accuracy: 0.9011\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 141s 187ms/step - loss: 0.2457 - accuracy: 0.9096 - val_loss: 0.2357 - val_accuracy: 0.9165\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 136s 181ms/step - loss: 0.2156 - accuracy: 0.9212 - val_loss: 0.2257 - val_accuracy: 0.9176\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 132s 176ms/step - loss: 0.1912 - accuracy: 0.9292 - val_loss: 0.2267 - val_accuracy: 0.9188\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 133s 178ms/step - loss: 0.1683 - accuracy: 0.9371 - val_loss: 0.2153 - val_accuracy: 0.9216\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 134s 178ms/step - loss: 0.1514 - accuracy: 0.9432 - val_loss: 0.2268 - val_accuracy: 0.9205\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 134s 178ms/step - loss: 0.1344 - accuracy: 0.9498 - val_loss: 0.2260 - val_accuracy: 0.9227\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 134s 179ms/step - loss: 0.1208 - accuracy: 0.9542 - val_loss: 0.2148 - val_accuracy: 0.9262\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 134s 179ms/step - loss: 0.1089 - accuracy: 0.9581 - val_loss: 0.2086 - val_accuracy: 0.9293\n",
            "313/313 [==============================] - 7s 22ms/step - loss: 0.2223 - accuracy: 0.9245\n",
            "Test accuracy: 0.92\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 271s 360ms/step - loss: 0.8761 - accuracy: 0.6922 - val_loss: 0.5962 - val_accuracy: 0.7837\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 270s 360ms/step - loss: 0.5728 - accuracy: 0.7872 - val_loss: 0.5288 - val_accuracy: 0.8011\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 270s 360ms/step - loss: 0.5071 - accuracy: 0.8172 - val_loss: 0.4905 - val_accuracy: 0.8233\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 273s 365ms/step - loss: 0.4586 - accuracy: 0.8359 - val_loss: 0.4579 - val_accuracy: 0.8425\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 269s 359ms/step - loss: 0.4255 - accuracy: 0.8484 - val_loss: 0.4311 - val_accuracy: 0.8453\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 268s 358ms/step - loss: 0.3968 - accuracy: 0.8600 - val_loss: 0.3822 - val_accuracy: 0.8657\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 271s 361ms/step - loss: 0.3742 - accuracy: 0.8663 - val_loss: 0.3677 - val_accuracy: 0.8697\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 269s 358ms/step - loss: 0.3550 - accuracy: 0.8741 - val_loss: 0.3585 - val_accuracy: 0.8742\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 267s 356ms/step - loss: 0.3390 - accuracy: 0.8802 - val_loss: 0.3348 - val_accuracy: 0.8823\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 269s 358ms/step - loss: 0.3278 - accuracy: 0.8831 - val_loss: 0.3334 - val_accuracy: 0.8816\n",
            "313/313 [==============================] - 15s 47ms/step - loss: 0.3505 - accuracy: 0.8760\n",
            "Test accuracy: 0.88\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 132s 175ms/step - loss: 2.2272 - accuracy: 0.1992 - val_loss: 2.0734 - val_accuracy: 0.3874\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 130s 174ms/step - loss: 1.6174 - accuracy: 0.4650 - val_loss: 1.0358 - val_accuracy: 0.6530\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 130s 173ms/step - loss: 1.0579 - accuracy: 0.5996 - val_loss: 0.8528 - val_accuracy: 0.6861\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 131s 174ms/step - loss: 0.9341 - accuracy: 0.6505 - val_loss: 0.7811 - val_accuracy: 0.7220\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 131s 175ms/step - loss: 0.8607 - accuracy: 0.6795 - val_loss: 0.7362 - val_accuracy: 0.7348\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 131s 175ms/step - loss: 0.8043 - accuracy: 0.7003 - val_loss: 0.6958 - val_accuracy: 0.7435\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 130s 173ms/step - loss: 0.7596 - accuracy: 0.7174 - val_loss: 0.6651 - val_accuracy: 0.7597\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 131s 175ms/step - loss: 0.7272 - accuracy: 0.7305 - val_loss: 0.6453 - val_accuracy: 0.7648\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 128s 171ms/step - loss: 0.6993 - accuracy: 0.7409 - val_loss: 0.6231 - val_accuracy: 0.7699\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 128s 170ms/step - loss: 0.6720 - accuracy: 0.7485 - val_loss: 0.6140 - val_accuracy: 0.7763\n",
            "313/313 [==============================] - 7s 24ms/step - loss: 0.6369 - accuracy: 0.7656\n",
            "Test accuracy: 0.77\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 278s 370ms/step - loss: 0.4296 - accuracy: 0.8465 - val_loss: 0.3220 - val_accuracy: 0.8820\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 274s 366ms/step - loss: 0.2780 - accuracy: 0.8992 - val_loss: 0.2726 - val_accuracy: 0.9007\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 275s 366ms/step - loss: 0.2254 - accuracy: 0.9178 - val_loss: 0.2467 - val_accuracy: 0.9098\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 275s 366ms/step - loss: 0.1904 - accuracy: 0.9290 - val_loss: 0.2238 - val_accuracy: 0.9186\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 273s 363ms/step - loss: 0.1601 - accuracy: 0.9409 - val_loss: 0.2225 - val_accuracy: 0.9211\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 273s 365ms/step - loss: 0.1318 - accuracy: 0.9512 - val_loss: 0.2320 - val_accuracy: 0.9196\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 271s 362ms/step - loss: 0.1114 - accuracy: 0.9589 - val_loss: 0.2231 - val_accuracy: 0.9237\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 272s 363ms/step - loss: 0.0895 - accuracy: 0.9673 - val_loss: 0.2311 - val_accuracy: 0.9268\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 276s 368ms/step - loss: 0.0748 - accuracy: 0.9727 - val_loss: 0.2485 - val_accuracy: 0.9264\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 273s 364ms/step - loss: 0.0598 - accuracy: 0.9775 - val_loss: 0.2642 - val_accuracy: 0.9273\n",
            "313/313 [==============================] - 15s 47ms/step - loss: 0.2862 - accuracy: 0.9206\n",
            "Test accuracy: 0.92\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 153s 202ms/step - loss: 0.6450 - accuracy: 0.7600 - val_loss: 0.4190 - val_accuracy: 0.8460\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 154s 206ms/step - loss: 0.4038 - accuracy: 0.8530 - val_loss: 0.3353 - val_accuracy: 0.8765\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 151s 202ms/step - loss: 0.3367 - accuracy: 0.8782 - val_loss: 0.3156 - val_accuracy: 0.8812\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 151s 202ms/step - loss: 0.2962 - accuracy: 0.8919 - val_loss: 0.2772 - val_accuracy: 0.8963\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 152s 202ms/step - loss: 0.2700 - accuracy: 0.9020 - val_loss: 0.2775 - val_accuracy: 0.8946\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 151s 202ms/step - loss: 0.2497 - accuracy: 0.9067 - val_loss: 0.2565 - val_accuracy: 0.9052\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 151s 201ms/step - loss: 0.2281 - accuracy: 0.9165 - val_loss: 0.2629 - val_accuracy: 0.8995\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 151s 202ms/step - loss: 0.2147 - accuracy: 0.9217 - val_loss: 0.2562 - val_accuracy: 0.9048\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 151s 201ms/step - loss: 0.1964 - accuracy: 0.9281 - val_loss: 0.2545 - val_accuracy: 0.9054\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 151s 201ms/step - loss: 0.1838 - accuracy: 0.9322 - val_loss: 0.2464 - val_accuracy: 0.9121\n",
            "313/313 [==============================] - 8s 26ms/step - loss: 0.2711 - accuracy: 0.9112\n",
            "Test accuracy: 0.91\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 395s 526ms/step - loss: 2.2678 - accuracy: 0.1844 - val_loss: 2.2146 - val_accuracy: 0.2952\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 414s 551ms/step - loss: 2.0550 - accuracy: 0.3553 - val_loss: 1.6161 - val_accuracy: 0.5860\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 396s 528ms/step - loss: 1.2703 - accuracy: 0.5467 - val_loss: 0.9472 - val_accuracy: 0.6719\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 414s 552ms/step - loss: 1.0022 - accuracy: 0.6297 - val_loss: 0.8312 - val_accuracy: 0.6966\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 397s 529ms/step - loss: 0.8958 - accuracy: 0.6714 - val_loss: 0.7618 - val_accuracy: 0.7197\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 397s 529ms/step - loss: 0.8210 - accuracy: 0.6987 - val_loss: 0.7096 - val_accuracy: 0.7458\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 397s 529ms/step - loss: 0.7660 - accuracy: 0.7182 - val_loss: 0.6772 - val_accuracy: 0.7464\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 413s 551ms/step - loss: 0.7249 - accuracy: 0.7311 - val_loss: 0.6443 - val_accuracy: 0.7628\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 397s 529ms/step - loss: 0.6947 - accuracy: 0.7403 - val_loss: 0.6226 - val_accuracy: 0.7668\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 414s 553ms/step - loss: 0.6657 - accuracy: 0.7520 - val_loss: 0.5990 - val_accuracy: 0.7797\n",
            "313/313 [==============================] - 22s 70ms/step - loss: 0.6190 - accuracy: 0.7694\n",
            "Test accuracy: 0.77\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 884s 1s/step - loss: 0.4069 - accuracy: 0.8554 - val_loss: 0.2838 - val_accuracy: 0.8987\n",
            "Epoch 2/10\n",
            "599/750 [======================>.......] - ETA: 2:41 - loss: 0.2682 - accuracy: 0.9014"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Function to create a CNN model\n",
        "def create_cnn_model(num_conv_layers, filters, kernel_size, pool_size, dropout_rate, optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters[0], kernel_size, activation='relu', input_shape=(28, 28, 1)))\n",
        "\n",
        "    # Reduce downsampling by adjusting pool size or removing some pooling layers\n",
        "    model.add(MaxPooling2D(pool_size=(1, 1)))  # Adjust pool size as needed\n",
        "\n",
        "    for i in range(1, num_conv_layers):\n",
        "        model.add(Conv2D(filters[i], kernel_size, activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=pool_size))  # Adjust pool size as needed\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Different configurations to try\n",
        "configs = [\n",
        "    (3, [32, 64, 128], (3, 3), (2, 2), 0.3, Adam(learning_rate=0.001)),\n",
        "    (2, [64, 128], (3, 3), (2, 2), 0.3, SGD(learning_rate=0.01)),\n",
        "    (3, [32, 64, 128], (3, 3), (2, 2), 0.3, Adam(learning_rate=0.001)),\n",
        "    (2, [64, 128], (3, 3), (2, 2), 0.3, SGD(learning_rate=0.01)),\n",
        "    (3, [32, 64, 128], (3, 3), (2, 2), 0.3, SGD(learning_rate=0.001)),\n",
        "    (2, [64, 128], (3, 3), (2, 2), 0.3, Adam(learning_rate=0.0005)),\n",
        "    (4, [32, 64, 128, 256], (3, 3), (2, 2), 0.4, Adam(learning_rate=0.001)),\n",
        "    (3, [64, 128, 256], (3, 3), (2, 2), 0.4, SGD(learning_rate=0.001)),\n",
        "    (2, [128, 256], (3, 3), (2, 2), 0.4, Adam(learning_rate=0.0005)),\n",
        "    (4, [64, 128, 256, 512], (3, 3), (2, 2), 0.5, SGD(learning_rate=0.001)),\n",
        "    (3, [32, 64, 128], (5, 5), (2, 2), 0.3, Adam(learning_rate=0.0001)),\n",
        "    (2, [64, 128], (5, 5), (2, 2), 0.3, SGD(learning_rate=0.001)),\n",
        "    (3, [32, 64, 128], (5, 5), (2, 2), 0.3, SGD(learning_rate=0.0005)),\n",
        "    (2, [64, 128], (5, 5), (2, 2), 0.3, Adam(learning_rate=0.0001)),\n",
        "    (4, [32, 64, 128, 256], (5, 5), (2, 2), 0.4, Adam(learning_rate=0.0005)),\n",
        "    (3, [64, 128, 256], (5, 5), (2, 2), 0.4, SGD(learning_rate=0.0005)),\n",
        "    (2, [128, 256], (5, 5), (2, 2), 0.4, Adam(learning_rate=0.0001)),\n",
        "    (4, [64, 128, 256, 512], (5, 5), (2, 2), 0.5, SGD(learning_rate=0.0005)),\n",
        "    (3, [32, 64, 128], (3, 3), (3, 3), 0.3, Adam(learning_rate=0.0001)),\n",
        "    (2, [64, 128], (3, 3), (3, 3), 0.3, SGD(learning_rate=0.001)),\n",
        "]\n",
        "\n",
        "# Evaluate each configuration\n",
        "for config in configs:\n",
        "    model = create_cnn_model(*config)\n",
        "    model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f'Test accuracy: {accuracy:.2f}')\n"
      ]
    }
  ]
}